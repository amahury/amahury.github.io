---
title: "Review | Semantic information, autonomous agency and non-equilibrium statistical physics"
date: 2025-11-14 00:00:00 -0400
categories: [Opinion]
tags: [review]
comments: true
toc: true 
math: true
pin: true 
mermaid: false
description: Shannon's information theory does not consider meaning; it is a quantitative theory focused on the probability and transmission of messages as signals, completely divorcing information from its semantic content. The paper I am reviewing today proposes a way, based on Shannon's approach, to measure the semantic aspects of systems.
alpez:
  name: Amahury J. L. Diaz
  twitter: amahury0
---
### Introduction
The current century has been defined, scientifically and culturally, by the concept of [information](https://en.wikipedia.org/wiki/Information). We have even [named an era](https://en.wikipedia.org/wiki/Information_Age) after it and [built entire sciences](https://en.wikipedia.org/wiki/Information_science) around it. Yet most of what we can measure cleanly is [_syntactic_ information](https://en.wikipedia.org/wiki/Syntax): statistical dependencies, code lengths, and entropies that say how well signals correlate with other signals. Shannon’s [information theory](https://en.wikipedia.org/wiki/Information_theory) turned this syntactic lens into an engineering powerhouse, but it deliberately bracketed [_meaning_](https://en.wikipedia.org/wiki/Meaning_(philosophy)). That omission continues to motivate proposals that try to connect information to [causation](https://en.wikipedia.org/wiki/Causality), [agency](https://en.wikipedia.org/wiki/Agency_(philosophy)), or [individuality](https://en.wikipedia.org/wiki/Individual), from [consciousness metrics](https://www.nature.com/articles/nrn.2016.44) to [evolutionary individuality indices](https://link.springer.com/article/10.1007/s12064-020-00313-7). The paper I am reviewing today joins this lineage by asking: when, and in what sense, do correlations _matter_ for a physical system?

[Kolchinsky and Wolpert’s answer](https://royalsocietypublishing.org/doi/full/10.1098/rsfs.2018.0041) is to define _semantic information_ as the portion of syntactic information that is _causally necessary_ for a system to keep existing. “Keeping existing,” in turn, is cashed out thermodynamically: a system maintains itself to the extent that it stays in a low-entropy, high-viability set of states. Operationally, they study what happens when we _scramble_ correlations between system and environment in counterfactual interventions and quantify how much the system’s viability goes up or down as a result. The correlations that cannot be scrambled without hurting viability count as [_semantic_](https://en.wikipedia.org/wiki/Semantics); the rest are mere [syntax](https://en.wikipedia.org/wiki/Syntax). This shift from description to counterfactual consequence is the paper’s central move.

The authors then ground their definitions in [non-equilibrium statistical physics](https://en.wikipedia.org/wiki/Statistical_mechanics#Non-equilibrium_statistical_mechanics), where the thermodynamics of information has shown that [reducing entropy](https://en.wikipedia.org/wiki/Maxwell%27s_demon) or [acquiring information](https://en.wikipedia.org/wiki/Landauer%27s_principle) carries energetic costs, while information can also deliver arbitrarily large benefits (e.g., guiding an agent to a cache of free energy). This lets them compare the “bang-per-bit” that correlations provide for self-maintenance, without committing to any particular biology or cognitive architecture. The result is a framework that aims to be substrate-agnostic: it should apply, in principle, to hurricanes, cells, or artifacts—any system we can carve from the world and track through time.

### Quantifying Meaning
The paper starts from a spare but consequential definition: semantic information is the syntactic information a system $X$ has about its environment $Y$ that is causally necessary for $X$ to maintain its existence over a time horizon $t$. “Causal necessity” is evaluated by counterfactual interventions that deliberately destroy (some of) the correlations between $X$ and $Y$ and then measure how the system’s viability changes. Viability is defined as negative Shannon entropy over $X$’s distribution at the evaluation time; lower entropy means higher probability of residing in any “viable set” of microstates, hence higher existence-value. The headline is elegant: not all correlations are equal, only those that make a difference to staying out of equilibrium count as semantic. 

The authors position their account against [teleosemantics](https://www.cambridge.org/core/books/how-biology-shapes-philosophy/teleosemantics/5CB3EBB3D20EB3FE76096D1122E8B7C5) and [exogenously imposed goals](https://en.wikipedia.org/wiki/Vitalism). Rather than stipulating what a system “ought” to maximize (utility, prediction accuracy), they treat self-maintenance as an intrinsic criterion: if a bit helps the system avoid thermodynamic death, it is meaningful for that system. This is conceptually attractive, because it avoids importing an observer’s purposes into the definition of meaning. It is also deliberately ahistorical: semantic value depends on the current joint distribution and dynamics, not on the trait’s evolutionary past. That choice makes the framework general enough to apply beyond Darwinian lineages (to protocells, devices, or synthetic agents), although it risks sidelining [path-dependence](https://en.wikipedia.org/wiki/Path_dependence) and [learning histories](https://en.wikipedia.org/wiki/Learning) that, in living systems, are often where meaning is made.

Technically, the setup decomposes the world into a system $X$ and an environment $Y$; fixes a timescale $t$; and specifies an initial joint distribution over $X_0$, $Y_0$. These are not trivial modeling choices, especially in biology, where system boundaries can be fuzzy (consider [holobionts](https://en.wikipedia.org/wiki/Holobiont) or [biofilms](https://en.wikipedia.org/wiki/Biofilm)). The authors acknowledge this and later suggest “objective” procedures (e.g., maximizing semantic measures) to choose a decomposition, horizon, and initial conditions, an idea that doubles as a recipe for agent detection: pick the carving and timescale for which semantic information is largest. This is, of course, still arbitrary, as [it is not clear](https://www.quantamagazine.org/why-are-plants-green-to-reduce-the-noise-in-photosynthesis-20200730/) why life should be following an utilitarian (maximization) principle to exist.

Finally, the framework draws explicitly on the [thermodynamics of information](https://www.nature.com/articles/nphys3230). Because information acquisition and entropy reduction both have energetic footprints, the authors define a _thermodynamic multiplier_: the ratio between the viability benefit of information and the number of bits. A multiplier greater than one means that each bit pays for itself—perhaps many times over—by enabling access to free energy in the environment. This provides a physically interpretable “return on information” that can be compared across systems.

### Semantic Flavors
A power of the framework is that the same interventionist recipe yields different flavors of semantic information depending on which syntactic quantity we scramble. The first flavor is stored _semantic information_. Here the intervention operates on the initial [mutual information](https://en.wikipedia.org/wiki/Mutual_information) $I(X_0; Y_0)$: we coarse-grain away parts of those correlations and ask how viability at time $t$ changes. The optimal intervention is the coarsest scrambling that preserves the system’s actual viability; whatever bits survive are, by definition, all semantic. This lets the authors define: (i) the amount of stored semantic information (how many bits survive in the optimal intervention); (ii) semantic efficiency (what fraction of the overall mutual information is truly semantic); and (iii) the semantic content of a state $x_0$ (the conditional distribution over $y_0$ that must be preserved for viability).

From there, they introduce the already mentioned _thermodynamic multiplier_ of stored semantic information: the viability benefit per bit relative to a physically motivated lower bound on the work needed to acquire those bits (Landauer-style). Intuitively, an agent that uses a few bits to harvest a large cache of [free energy](https://en.wikipedia.org/wiki/Thermodynamic_free_energy) has a very high multiplier; an agent that hoards correlations it cannot use has a low one. Importantly, the authors flag that this accounting _ignores interaction energies_ and other real-world costs; extending the analysis to energetically constrained interventions is actually an open problem.

The second flavor is observed semantic information, which tracks dynamically acquired bits via [transfer entropy](https://en.wikipedia.org/wiki/Transfer_entropy) from $Y_t$ to $X_{t+1}$ ​ (conditioned on $X_t$). Instead of scrambling the initial joint state, the intervention perturbs the flow of information during the dynamics while keeping the starting distribution fixed. Applying the same optimal-intervention logic yields an amount, an efficiency, and a content for the observed case, as well as a (still future-work) thermodynamic multiplier that would compare viability benefit to the true energetic cost of measurement/observation. This “observed” notion captures what an agent learns in real time from coupling with its environment.

At this point, two conceptual clarifications matter. First, semantic information is asymmetrical by construction. Even when the underlying syntactic measure is symmetric (e.g., [mutual information](https://en.wikipedia.org/wiki/Mutual_information)), the semantic attribution runs from _environment to system_, because viability is assessed for the system only. That is exactly what we want from a notion tied to [_biological agency_](https://academic.oup.com/jeb/article/38/2/143/7920097). Second, semantic bits can be helpful or harmful: viability changes can be positive or negative under intervention, and the framework cleanly distinguishes “attention to the right bits” (high efficiency, high multiplier) from “misleading correlations.”

Beyond across-boundary correlations, the same recipe can be applied _within_ the system (e.g., between internal subsystems) to study homeostatic control or metabolic regulation. It can also be reversed (system-to-environment) to quantify cases where the _environment_’s “observation” of the system affects the system’s viability (as with [closed-loop chemical organizations](https://arxiv.org/abs/2404.04374)). This generality hints at a unifying calculus for [informational constraints in living organization](https://www.sciencedirect.com/science/article/pii/S0022519315001009). 

### Conclusions
Kolchinsky and Wolpert set out to pull “meaning” into physics without hand-waving. They do so by marrying counterfactual interventions with non-equilibrium thermodynamics: call _semantic_ only that portion of syntactic information whose removal predictably reduces a system’s ability to remain in low-entropy states. The framework’s measures depend on three modeling choices: the system/environment decomposition, the timescale, and the initial distribution, but the authors suggest principled ways to select them (e.g., maximize semantic information), turning an apparent subjectivity into a tool for _agent detection_. This also yields a crisp criterion for _autonomous agency_: systems with large, efficiency-weighted semantic stores (and flows) count as more agent-like.

Several open gaps remain, and they are important. First, energetic realism: the thermodynamic multiplier is introduced with idealized measurement costs and explicitly ignores interaction energies and the energetic side-effects of the interventions themselves. Extending the analysis to energetically constrained interventions, non-negligible coupling Hamiltonians, and measured costs (e.g., ATP usage) would strengthen the physical grounding and sharpen the link to work extraction. Second, history and learning: by design the framework is present-tense and dynamics-based, [not etiological](https://en.wikipedia.org/wiki/Etiology). That gives it generality, but living meaning is often made by [_path-dependence_](https://en.wikipedia.org/wiki/Path_dependence). Connecting semantic information to longer temporal windows (e.g., across growth and learning) seems both natural and necessary.

Third, where to cut the world. The promise of “objective” carving by maximizing semantic quantities is intriguing, but real systems (from [symbioses](https://en.wikipedia.org/wiki/Symbiosis) to [collectives](https://en.wikipedia.org/wiki/Collective)) may present _nested_ agents with overlapping timescales. Here the framework’s asymmetry is an asset: because semantic information is always “for the system,” one can search over decompositions and horizons to uncover the levels at which viability-supporting correlations concentrate. In fact, there are resonances worth developing: the maximization procedures rhyme with active-inference and [free-energy-principle stories](https://en.wikipedia.org/wiki/Free_energy_principle), yet the present framework stays agnostic about internal models, sensors, or effectors—a virtue for messy biology. Taken together, this paper supplies a physically disciplined vocabulary for talking about meaning, autonomy, and life-likeness—one that invites, rather than forecloses, empirical tests.

If a test for any theory of semantic information is whether it helps us identify and compare _what matters_ for persistence, this intervention-based approach passes the smell test. It does not solve, however, the [metaphysics of meaning](https://mitpress.mit.edu/9780262610827/the-metaphysics-of-meaning/) neither provide hits on the [epistemological irreducibility](https://casci.binghamton.edu/publications/pattee/pattee.html) of the [three biosemiotic axes](https://petercariani.com/Cybernetics_files/CarianiPhDIntegrated1989.pdf); it does something more modest and more useful: it tells us how to ask, in bits and joules, which correlations are _worth keeping_. 
 
![Desktop View](/assets/img/fix/complexity-cat-newsletter.png){: .normal width="1200" height="630" }
